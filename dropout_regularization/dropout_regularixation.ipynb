{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a267d9",
   "metadata": {},
   "source": [
    "### Importing required dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a584cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "32712027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10645155",
   "metadata": {},
   "source": [
    "#### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76943f6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>0.6415</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.7547</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.6097</td>\n",
       "      <td>0.4943</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.7464</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.8024</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>0.4052</td>\n",
       "      <td>0.3957</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.3271</td>\n",
       "      <td>0.2767</td>\n",
       "      <td>0.4423</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.4182</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.2129</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.3973</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.4135</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6   ...      54      55      56      57      58      59  60\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  ...  0.0072  0.0167  0.0180  0.0084  0.0090  0.0032   R\n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  ...  0.0094  0.0191  0.0140  0.0049  0.0052  0.0044   R\n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  ...  0.0180  0.0244  0.0316  0.0164  0.0095  0.0078   R\n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  ...  0.0085  0.0073  0.0050  0.0044  0.0040  0.0117   R\n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  ...  0.0110  0.0015  0.0072  0.0048  0.0107  0.0094   R\n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar = pd.read_csv(\"files/sonar_dataset.csv\", header= None)\n",
    "sonar.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bf312e",
   "metadata": {},
   "source": [
    "## Replaceing M-->1  and  R-->0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce49b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar.replace({\"R\": 0, \"M\": 1}, inplace= True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1b3bdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.1609</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>0.2238</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>0.0660</td>\n",
       "      <td>0.2273</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.5078</td>\n",
       "      <td>0.4797</td>\n",
       "      <td>0.5783</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>0.4328</td>\n",
       "      <td>0.5550</td>\n",
       "      <td>0.6711</td>\n",
       "      <td>0.6415</td>\n",
       "      <td>0.7104</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.5121</td>\n",
       "      <td>0.7547</td>\n",
       "      <td>0.8537</td>\n",
       "      <td>0.8507</td>\n",
       "      <td>0.6692</td>\n",
       "      <td>0.6097</td>\n",
       "      <td>0.4943</td>\n",
       "      <td>0.2744</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>0.2825</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>0.1386</td>\n",
       "      <td>0.1051</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.4918</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>0.7797</td>\n",
       "      <td>0.7464</td>\n",
       "      <td>0.9444</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8874</td>\n",
       "      <td>0.8024</td>\n",
       "      <td>0.7818</td>\n",
       "      <td>0.5212</td>\n",
       "      <td>0.4052</td>\n",
       "      <td>0.3957</td>\n",
       "      <td>0.3914</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.3200</td>\n",
       "      <td>0.3271</td>\n",
       "      <td>0.2767</td>\n",
       "      <td>0.4423</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.3788</td>\n",
       "      <td>0.2947</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.4182</td>\n",
       "      <td>0.3835</td>\n",
       "      <td>0.1057</td>\n",
       "      <td>0.1840</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.0742</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>0.7060</td>\n",
       "      <td>0.5544</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>0.6479</td>\n",
       "      <td>0.6931</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.7551</td>\n",
       "      <td>0.8929</td>\n",
       "      <td>0.8619</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>0.6737</td>\n",
       "      <td>0.4293</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>0.5331</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>0.8533</td>\n",
       "      <td>0.6036</td>\n",
       "      <td>0.8514</td>\n",
       "      <td>0.8512</td>\n",
       "      <td>0.5045</td>\n",
       "      <td>0.1862</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>0.6116</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.5375</td>\n",
       "      <td>0.4719</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.2129</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1992</td>\n",
       "      <td>0.0184</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.2281</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.3973</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4846</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.5334</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>0.2520</td>\n",
       "      <td>0.2090</td>\n",
       "      <td>0.3559</td>\n",
       "      <td>0.6260</td>\n",
       "      <td>0.7340</td>\n",
       "      <td>0.6120</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.3953</td>\n",
       "      <td>0.3012</td>\n",
       "      <td>0.5408</td>\n",
       "      <td>0.8814</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>0.5006</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>0.3202</td>\n",
       "      <td>0.4295</td>\n",
       "      <td>0.3654</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>0.0681</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>0.4152</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>0.4256</td>\n",
       "      <td>0.4135</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.6193</td>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.4636</td>\n",
       "      <td>0.4148</td>\n",
       "      <td>0.4292</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>0.5399</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.6995</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7262</td>\n",
       "      <td>0.4724</td>\n",
       "      <td>0.5103</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.2881</td>\n",
       "      <td>0.0981</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.3217</td>\n",
       "      <td>0.2828</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.1979</td>\n",
       "      <td>0.2444</td>\n",
       "      <td>0.1847</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.0692</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6   ...      54      55      56      57      58      59  60\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  ...  0.0072  0.0167  0.0180  0.0084  0.0090  0.0032   0\n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  ...  0.0094  0.0191  0.0140  0.0049  0.0052  0.0044   0\n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  ...  0.0180  0.0244  0.0316  0.0164  0.0095  0.0078   0\n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  ...  0.0085  0.0073  0.0050  0.0044  0.0040  0.0117   0\n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  ...  0.0110  0.0015  0.0072  0.0048  0.0107  0.0094   0\n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f21e6",
   "metadata": {},
   "source": [
    "### Looking for missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2cf789",
   "metadata": {},
   "source": [
    "1. Checkong the shape and dtype of every row so we can find missing value accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c220666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 61)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "846fd43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    60\n",
       "int64       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9b707",
   "metadata": {},
   "source": [
    "-- So there is no categorical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a980339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208.0    61\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar.describe().iloc[0].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a386659",
   "metadata": {},
   "source": [
    "so there is 208 cousts in all(61) feature in .describe dataframe and also there is 61 feature at all so there is no missing value in the dataframe at first glannce but might be possible if any has just space value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49306e88",
   "metadata": {},
   "source": [
    "### Train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57284669",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sonar.drop(60, axis= \"columns\")\n",
    "y = sonar[60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a5e218c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0664</td>\n",
       "      <td>0.0575</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.1130</td>\n",
       "      <td>0.2353</td>\n",
       "      <td>0.1838</td>\n",
       "      <td>0.2869</td>\n",
       "      <td>0.4129</td>\n",
       "      <td>0.3647</td>\n",
       "      <td>0.1984</td>\n",
       "      <td>0.2840</td>\n",
       "      <td>0.4039</td>\n",
       "      <td>0.5837</td>\n",
       "      <td>0.6792</td>\n",
       "      <td>0.6086</td>\n",
       "      <td>0.4858</td>\n",
       "      <td>0.3246</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2082</td>\n",
       "      <td>0.1686</td>\n",
       "      <td>0.2484</td>\n",
       "      <td>0.2736</td>\n",
       "      <td>0.2984</td>\n",
       "      <td>0.4655</td>\n",
       "      <td>0.6990</td>\n",
       "      <td>0.7474</td>\n",
       "      <td>0.7956</td>\n",
       "      <td>0.7981</td>\n",
       "      <td>0.6715</td>\n",
       "      <td>0.6942</td>\n",
       "      <td>0.7440</td>\n",
       "      <td>0.8169</td>\n",
       "      <td>0.8912</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8753</td>\n",
       "      <td>0.7061</td>\n",
       "      <td>0.6803</td>\n",
       "      <td>0.5898</td>\n",
       "      <td>0.4618</td>\n",
       "      <td>0.3639</td>\n",
       "      <td>0.1492</td>\n",
       "      <td>0.1216</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.1198</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.0079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0530</td>\n",
       "      <td>0.1197</td>\n",
       "      <td>0.1243</td>\n",
       "      <td>0.1026</td>\n",
       "      <td>0.1239</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.1599</td>\n",
       "      <td>0.1542</td>\n",
       "      <td>0.1846</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.1748</td>\n",
       "      <td>0.1455</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>0.2257</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>0.3368</td>\n",
       "      <td>0.5828</td>\n",
       "      <td>0.8505</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8457</td>\n",
       "      <td>0.6624</td>\n",
       "      <td>0.5564</td>\n",
       "      <td>0.3925</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>0.2054</td>\n",
       "      <td>0.1920</td>\n",
       "      <td>0.2227</td>\n",
       "      <td>0.3147</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.0795</td>\n",
       "      <td>0.0748</td>\n",
       "      <td>0.1166</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.2619</td>\n",
       "      <td>0.2507</td>\n",
       "      <td>0.1983</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>0.0381</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0323</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.0760</td>\n",
       "      <td>0.0958</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2154</td>\n",
       "      <td>0.3085</td>\n",
       "      <td>0.3425</td>\n",
       "      <td>0.2990</td>\n",
       "      <td>0.1402</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>0.1534</td>\n",
       "      <td>0.1901</td>\n",
       "      <td>0.2429</td>\n",
       "      <td>0.2120</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.3272</td>\n",
       "      <td>0.5949</td>\n",
       "      <td>0.8302</td>\n",
       "      <td>0.9045</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>0.9912</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9092</td>\n",
       "      <td>0.7412</td>\n",
       "      <td>0.7691</td>\n",
       "      <td>0.7117</td>\n",
       "      <td>0.5304</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.1297</td>\n",
       "      <td>0.1159</td>\n",
       "      <td>0.1226</td>\n",
       "      <td>0.1768</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.1562</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.1149</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.0788</td>\n",
       "      <td>0.0766</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.1594</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>0.2652</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2381</td>\n",
       "      <td>0.1918</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>0.1781</td>\n",
       "      <td>0.2852</td>\n",
       "      <td>0.5036</td>\n",
       "      <td>0.6166</td>\n",
       "      <td>0.7616</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.7793</td>\n",
       "      <td>0.8788</td>\n",
       "      <td>0.8813</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>0.8446</td>\n",
       "      <td>0.6151</td>\n",
       "      <td>0.4302</td>\n",
       "      <td>0.3165</td>\n",
       "      <td>0.2869</td>\n",
       "      <td>0.2017</td>\n",
       "      <td>0.1206</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>0.0580</td>\n",
       "      <td>0.1262</td>\n",
       "      <td>0.1072</td>\n",
       "      <td>0.1082</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.1197</td>\n",
       "      <td>0.2061</td>\n",
       "      <td>0.2054</td>\n",
       "      <td>0.1878</td>\n",
       "      <td>0.2047</td>\n",
       "      <td>0.1716</td>\n",
       "      <td>0.1069</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.1787</td>\n",
       "      <td>0.1635</td>\n",
       "      <td>0.0887</td>\n",
       "      <td>0.0817</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.2053</td>\n",
       "      <td>0.3135</td>\n",
       "      <td>0.3118</td>\n",
       "      <td>0.3686</td>\n",
       "      <td>0.3885</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>0.7868</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.8610</td>\n",
       "      <td>0.8443</td>\n",
       "      <td>0.9061</td>\n",
       "      <td>0.5847</td>\n",
       "      <td>0.4033</td>\n",
       "      <td>0.5946</td>\n",
       "      <td>0.6793</td>\n",
       "      <td>0.6389</td>\n",
       "      <td>0.5002</td>\n",
       "      <td>0.5578</td>\n",
       "      <td>0.4831</td>\n",
       "      <td>0.4729</td>\n",
       "      <td>0.3318</td>\n",
       "      <td>0.3969</td>\n",
       "      <td>0.3894</td>\n",
       "      <td>0.2314</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.1312</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.2569</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.2649</td>\n",
       "      <td>0.2714</td>\n",
       "      <td>0.1713</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.2198</td>\n",
       "      <td>0.1074</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.0630</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>0.1368</td>\n",
       "      <td>0.1078</td>\n",
       "      <td>0.1552</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.2164</td>\n",
       "      <td>0.2568</td>\n",
       "      <td>0.3089</td>\n",
       "      <td>0.3829</td>\n",
       "      <td>0.4393</td>\n",
       "      <td>0.5335</td>\n",
       "      <td>0.5996</td>\n",
       "      <td>0.6728</td>\n",
       "      <td>0.7309</td>\n",
       "      <td>0.8092</td>\n",
       "      <td>0.8941</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9893</td>\n",
       "      <td>0.9376</td>\n",
       "      <td>0.8991</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>0.9128</td>\n",
       "      <td>0.7811</td>\n",
       "      <td>0.6018</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.1788</td>\n",
       "      <td>0.2373</td>\n",
       "      <td>0.2843</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>0.2715</td>\n",
       "      <td>0.3363</td>\n",
       "      <td>0.2546</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.2160</td>\n",
       "      <td>0.1278</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.0946</td>\n",
       "      <td>0.0636</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6   ...      53      54      55      56      57      58      59\n",
       "21   0.0664  0.0575  0.0842  0.0372  0.0458  0.0771  0.0771  ...  0.0043  0.0036  0.0026  0.0024  0.0162  0.0109  0.0079\n",
       "71   0.0036  0.0078  0.0092  0.0387  0.0530  0.1197  0.1243  ...  0.0035  0.0036  0.0004  0.0018  0.0049  0.0024  0.0016\n",
       "204  0.0323  0.0101  0.0298  0.0564  0.0760  0.0958  0.0990  ...  0.0135  0.0063  0.0063  0.0034  0.0032  0.0062  0.0067\n",
       "198  0.0238  0.0318  0.0422  0.0399  0.0788  0.0766  0.0881  ...  0.0084  0.0038  0.0026  0.0028  0.0013  0.0035  0.0060\n",
       "84   0.0378  0.0318  0.0423  0.0350  0.1787  0.1635  0.0887  ...  0.0078  0.0102  0.0065  0.0061  0.0062  0.0043  0.0053\n",
       "121  0.0162  0.0041  0.0239  0.0441  0.0630  0.0921  0.1368  ...  0.0114  0.0062  0.0157  0.0088  0.0036  0.0053  0.0030\n",
       "\n",
       "[6 rows x 60 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75affbf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98     1\n",
       "171    1\n",
       "69     0\n",
       "8      0\n",
       "0      0\n",
       "181    1\n",
       "Name: 60, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7ca509e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "203    1\n",
       "204    1\n",
       "205    1\n",
       "206    1\n",
       "207    1\n",
       "Name: 60, Length: 208, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d523810",
   "metadata": {},
   "source": [
    "--> it is not shuffled, but it need to be shuffled because we feeding class in regular way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e1b9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03fcac6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0953</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.1424</td>\n",
       "      <td>0.1972</td>\n",
       "      <td>0.1873</td>\n",
       "      <td>0.1806</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.1523</td>\n",
       "      <td>0.1975</td>\n",
       "      <td>0.4844</td>\n",
       "      <td>0.7298</td>\n",
       "      <td>0.7807</td>\n",
       "      <td>0.7906</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.5148</td>\n",
       "      <td>0.7569</td>\n",
       "      <td>0.8596</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8457</td>\n",
       "      <td>0.6797</td>\n",
       "      <td>0.6971</td>\n",
       "      <td>0.5843</td>\n",
       "      <td>0.4772</td>\n",
       "      <td>0.5201</td>\n",
       "      <td>0.4241</td>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.1668</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.3967</td>\n",
       "      <td>0.7147</td>\n",
       "      <td>0.7319</td>\n",
       "      <td>0.3509</td>\n",
       "      <td>0.0589</td>\n",
       "      <td>0.2690</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.3874</td>\n",
       "      <td>0.2440</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.2307</td>\n",
       "      <td>0.1886</td>\n",
       "      <td>0.1960</td>\n",
       "      <td>0.1701</td>\n",
       "      <td>0.1366</td>\n",
       "      <td>0.0398</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.2684</td>\n",
       "      <td>0.3108</td>\n",
       "      <td>0.2933</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>0.0994</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2034</td>\n",
       "      <td>0.1740</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>0.6879</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>0.8453</td>\n",
       "      <td>0.8919</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8104</td>\n",
       "      <td>0.6199</td>\n",
       "      <td>0.6041</td>\n",
       "      <td>0.5547</td>\n",
       "      <td>0.4160</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>0.1411</td>\n",
       "      <td>0.1676</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>0.1977</td>\n",
       "      <td>0.1339</td>\n",
       "      <td>0.0902</td>\n",
       "      <td>0.1085</td>\n",
       "      <td>0.1521</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>0.0858</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0101</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.0403</td>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0820</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.1161</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.0906</td>\n",
       "      <td>0.2545</td>\n",
       "      <td>0.1464</td>\n",
       "      <td>0.1272</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.1424</td>\n",
       "      <td>0.1285</td>\n",
       "      <td>0.1857</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.2547</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.1923</td>\n",
       "      <td>0.4753</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>0.6825</td>\n",
       "      <td>0.6443</td>\n",
       "      <td>0.7063</td>\n",
       "      <td>0.5373</td>\n",
       "      <td>0.6601</td>\n",
       "      <td>0.8708</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>0.9605</td>\n",
       "      <td>0.7712</td>\n",
       "      <td>0.6772</td>\n",
       "      <td>0.6431</td>\n",
       "      <td>0.6720</td>\n",
       "      <td>0.6035</td>\n",
       "      <td>0.5155</td>\n",
       "      <td>0.3802</td>\n",
       "      <td>0.2278</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.0801</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>0.0566</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1054</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.2302</td>\n",
       "      <td>0.2259</td>\n",
       "      <td>0.2373</td>\n",
       "      <td>0.3323</td>\n",
       "      <td>0.3827</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>0.6812</td>\n",
       "      <td>0.7555</td>\n",
       "      <td>0.9522</td>\n",
       "      <td>0.9826</td>\n",
       "      <td>0.8871</td>\n",
       "      <td>0.8268</td>\n",
       "      <td>0.7561</td>\n",
       "      <td>0.8217</td>\n",
       "      <td>0.6967</td>\n",
       "      <td>0.6444</td>\n",
       "      <td>0.6948</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>0.6053</td>\n",
       "      <td>0.6084</td>\n",
       "      <td>0.8877</td>\n",
       "      <td>0.8557</td>\n",
       "      <td>0.5563</td>\n",
       "      <td>0.2897</td>\n",
       "      <td>0.3638</td>\n",
       "      <td>0.4786</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.2043</td>\n",
       "      <td>0.1707</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.1286</td>\n",
       "      <td>0.1581</td>\n",
       "      <td>0.2191</td>\n",
       "      <td>0.1701</td>\n",
       "      <td>0.0971</td>\n",
       "      <td>0.2217</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.1874</td>\n",
       "      <td>0.1062</td>\n",
       "      <td>0.0665</td>\n",
       "      <td>0.0405</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0061</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.1171</td>\n",
       "      <td>0.1257</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.1258</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>0.2716</td>\n",
       "      <td>0.2374</td>\n",
       "      <td>0.1878</td>\n",
       "      <td>0.0983</td>\n",
       "      <td>0.0683</td>\n",
       "      <td>0.1503</td>\n",
       "      <td>0.1723</td>\n",
       "      <td>0.2339</td>\n",
       "      <td>0.1962</td>\n",
       "      <td>0.1395</td>\n",
       "      <td>0.3164</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.7631</td>\n",
       "      <td>0.8473</td>\n",
       "      <td>0.9424</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.8630</td>\n",
       "      <td>0.6979</td>\n",
       "      <td>0.7717</td>\n",
       "      <td>0.7305</td>\n",
       "      <td>0.5197</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1446</td>\n",
       "      <td>0.1066</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.1490</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.0910</td>\n",
       "      <td>0.0757</td>\n",
       "      <td>0.1059</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.0535</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0160</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6   ...      53      54      55      56      57      58      59\n",
       "86   0.0188  0.0370  0.0953  0.0824  0.0249  0.0488  0.1424  ...  0.0113  0.0030  0.0057  0.0090  0.0057  0.0068  0.0024\n",
       "203  0.0187  0.0346  0.0168  0.0177  0.0393  0.1630  0.2028  ...  0.0199  0.0033  0.0101  0.0065  0.0115  0.0193  0.0157\n",
       "67   0.0368  0.0403  0.0317  0.0293  0.0820  0.1342  0.1161  ...  0.0160  0.0081  0.0070  0.0135  0.0067  0.0078  0.0068\n",
       "82   0.0409  0.0421  0.0573  0.0130  0.0183  0.1019  0.1054  ...  0.0105  0.0120  0.0087  0.0061  0.0061  0.0030  0.0078\n",
       "205  0.0522  0.0437  0.0180  0.0292  0.0351  0.1171  0.1257  ...  0.0051  0.0062  0.0089  0.0140  0.0138  0.0077  0.0031\n",
       "\n",
       "[5 rows x 60 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d046a02f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86     0\n",
       "203    1\n",
       "67     0\n",
       "82     0\n",
       "205    1\n",
       "Name: 60, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3633cffa",
   "metadata": {},
   "source": [
    "### Creating ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed55dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "        keras.layers.Dense(20 ,input_shape = (60, ), activation = \"relu\"),\n",
    "        keras.layers.Dense(1, activation= \"sigmoid\")\n",
    "    ])\n",
    "    \n",
    "model.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = [\"accuracy\"]  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da25ec",
   "metadata": {},
   "source": [
    "1. model with dropout constrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c9e834f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 166 samples\n",
      "Epoch 1/100\n",
      "166/166 [==============================] - 0s 110us/sample - loss: 0.6204 - accuracy: 0.7229\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - 0s 162us/sample - loss: 0.6186 - accuracy: 0.7229\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - 0s 134us/sample - loss: 0.6133 - accuracy: 0.7229\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - 0s 131us/sample - loss: 0.6104 - accuracy: 0.7169\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - 0s 117us/sample - loss: 0.6086 - accuracy: 0.7108\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - 0s 140us/sample - loss: 0.6033 - accuracy: 0.7349\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - 0s 134us/sample - loss: 0.6018 - accuracy: 0.7410\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - 0s 130us/sample - loss: 0.5987 - accuracy: 0.7410\n",
      "Epoch 9/100\n",
      "166/166 [==============================] - 0s 143us/sample - loss: 0.5947 - accuracy: 0.7530\n",
      "Epoch 10/100\n",
      "166/166 [==============================] - 0s 151us/sample - loss: 0.5899 - accuracy: 0.7470\n",
      "Epoch 11/100\n",
      "166/166 [==============================] - 0s 115us/sample - loss: 0.5864 - accuracy: 0.7590\n",
      "Epoch 12/100\n",
      "166/166 [==============================] - 0s 125us/sample - loss: 0.5824 - accuracy: 0.7530\n",
      "Epoch 13/100\n",
      "166/166 [==============================] - 0s 137us/sample - loss: 0.5789 - accuracy: 0.7530\n",
      "Epoch 14/100\n",
      "166/166 [==============================] - 0s 126us/sample - loss: 0.5762 - accuracy: 0.7711\n",
      "Epoch 15/100\n",
      "166/166 [==============================] - 0s 121us/sample - loss: 0.5728 - accuracy: 0.7651\n",
      "Epoch 16/100\n",
      "166/166 [==============================] - 0s 129us/sample - loss: 0.5685 - accuracy: 0.7771\n",
      "Epoch 17/100\n",
      "166/166 [==============================] - 0s 137us/sample - loss: 0.5683 - accuracy: 0.7831\n",
      "Epoch 18/100\n",
      "166/166 [==============================] - 0s 113us/sample - loss: 0.5642 - accuracy: 0.7590\n",
      "Epoch 19/100\n",
      "166/166 [==============================] - 0s 133us/sample - loss: 0.5599 - accuracy: 0.7651\n",
      "Epoch 20/100\n",
      "166/166 [==============================] - 0s 133us/sample - loss: 0.5565 - accuracy: 0.7892\n",
      "Epoch 21/100\n",
      "166/166 [==============================] - 0s 130us/sample - loss: 0.5535 - accuracy: 0.7711\n",
      "Epoch 22/100\n",
      "166/166 [==============================] - 0s 125us/sample - loss: 0.5494 - accuracy: 0.7952\n",
      "Epoch 23/100\n",
      "166/166 [==============================] - 0s 124us/sample - loss: 0.5463 - accuracy: 0.7892\n",
      "Epoch 24/100\n",
      "166/166 [==============================] - 0s 122us/sample - loss: 0.5431 - accuracy: 0.7892\n",
      "Epoch 25/100\n",
      "166/166 [==============================] - 0s 141us/sample - loss: 0.5406 - accuracy: 0.7771\n",
      "Epoch 26/100\n",
      "166/166 [==============================] - 0s 118us/sample - loss: 0.5374 - accuracy: 0.7892\n",
      "Epoch 27/100\n",
      "166/166 [==============================] - 0s 113us/sample - loss: 0.5337 - accuracy: 0.7711\n",
      "Epoch 28/100\n",
      "166/166 [==============================] - 0s 118us/sample - loss: 0.5306 - accuracy: 0.7771\n",
      "Epoch 29/100\n",
      "166/166 [==============================] - 0s 108us/sample - loss: 0.5269 - accuracy: 0.7831\n",
      "Epoch 30/100\n",
      "166/166 [==============================] - 0s 114us/sample - loss: 0.5237 - accuracy: 0.7952\n",
      "Epoch 31/100\n",
      "166/166 [==============================] - 0s 108us/sample - loss: 0.5208 - accuracy: 0.7952\n",
      "Epoch 32/100\n",
      "166/166 [==============================] - 0s 139us/sample - loss: 0.5170 - accuracy: 0.8072\n",
      "Epoch 33/100\n",
      "166/166 [==============================] - 0s 111us/sample - loss: 0.5184 - accuracy: 0.7831\n",
      "Epoch 34/100\n",
      "166/166 [==============================] - 0s 141us/sample - loss: 0.5170 - accuracy: 0.7831\n",
      "Epoch 35/100\n",
      "166/166 [==============================] - 0s 135us/sample - loss: 0.5135 - accuracy: 0.7771\n",
      "Epoch 36/100\n",
      "166/166 [==============================] - 0s 163us/sample - loss: 0.5080 - accuracy: 0.8012\n",
      "Epoch 37/100\n",
      "166/166 [==============================] - 0s 134us/sample - loss: 0.5054 - accuracy: 0.7952\n",
      "Epoch 38/100\n",
      "166/166 [==============================] - 0s 127us/sample - loss: 0.5029 - accuracy: 0.8193\n",
      "Epoch 39/100\n",
      "166/166 [==============================] - 0s 135us/sample - loss: 0.4998 - accuracy: 0.8072\n",
      "Epoch 40/100\n",
      "166/166 [==============================] - 0s 114us/sample - loss: 0.4981 - accuracy: 0.8072\n",
      "Epoch 41/100\n",
      "166/166 [==============================] - 0s 98us/sample - loss: 0.4973 - accuracy: 0.7952\n",
      "Epoch 42/100\n",
      "166/166 [==============================] - 0s 125us/sample - loss: 0.4927 - accuracy: 0.8133\n",
      "Epoch 43/100\n",
      "166/166 [==============================] - 0s 114us/sample - loss: 0.4891 - accuracy: 0.8193\n",
      "Epoch 44/100\n",
      "166/166 [==============================] - 0s 120us/sample - loss: 0.4868 - accuracy: 0.8133\n",
      "Epoch 45/100\n",
      "166/166 [==============================] - 0s 121us/sample - loss: 0.4838 - accuracy: 0.8253\n",
      "Epoch 46/100\n",
      "166/166 [==============================] - 0s 120us/sample - loss: 0.4823 - accuracy: 0.8072\n",
      "Epoch 47/100\n",
      "166/166 [==============================] - 0s 122us/sample - loss: 0.4806 - accuracy: 0.8133\n",
      "Epoch 48/100\n",
      "166/166 [==============================] - 0s 100us/sample - loss: 0.4769 - accuracy: 0.8193\n",
      "Epoch 49/100\n",
      "166/166 [==============================] - 0s 116us/sample - loss: 0.4741 - accuracy: 0.8253\n",
      "Epoch 50/100\n",
      "166/166 [==============================] - 0s 137us/sample - loss: 0.4733 - accuracy: 0.7952\n",
      "Epoch 51/100\n",
      "166/166 [==============================] - 0s 138us/sample - loss: 0.4706 - accuracy: 0.7952\n",
      "Epoch 52/100\n",
      "166/166 [==============================] - 0s 128us/sample - loss: 0.4678 - accuracy: 0.8193\n",
      "Epoch 53/100\n",
      "166/166 [==============================] - 0s 178us/sample - loss: 0.4666 - accuracy: 0.8373\n",
      "Epoch 54/100\n",
      "166/166 [==============================] - 0s 163us/sample - loss: 0.4659 - accuracy: 0.8133\n",
      "Epoch 55/100\n",
      "166/166 [==============================] - 0s 122us/sample - loss: 0.4634 - accuracy: 0.8072\n",
      "Epoch 56/100\n",
      "166/166 [==============================] - 0s 109us/sample - loss: 0.4625 - accuracy: 0.8253\n",
      "Epoch 57/100\n",
      "166/166 [==============================] - 0s 119us/sample - loss: 0.4570 - accuracy: 0.8253\n",
      "Epoch 58/100\n",
      "166/166 [==============================] - 0s 90us/sample - loss: 0.4545 - accuracy: 0.8253\n",
      "Epoch 59/100\n",
      "166/166 [==============================] - 0s 95us/sample - loss: 0.4527 - accuracy: 0.8253\n",
      "Epoch 60/100\n",
      "166/166 [==============================] - 0s 108us/sample - loss: 0.4494 - accuracy: 0.8313\n",
      "Epoch 61/100\n",
      "166/166 [==============================] - 0s 111us/sample - loss: 0.4484 - accuracy: 0.8373\n",
      "Epoch 62/100\n",
      "166/166 [==============================] - 0s 150us/sample - loss: 0.4460 - accuracy: 0.8434\n",
      "Epoch 63/100\n",
      "166/166 [==============================] - 0s 108us/sample - loss: 0.4441 - accuracy: 0.8434\n",
      "Epoch 64/100\n",
      "166/166 [==============================] - 0s 91us/sample - loss: 0.4418 - accuracy: 0.8373\n",
      "Epoch 65/100\n",
      "166/166 [==============================] - 0s 122us/sample - loss: 0.4398 - accuracy: 0.8373\n",
      "Epoch 66/100\n",
      "166/166 [==============================] - 0s 120us/sample - loss: 0.4378 - accuracy: 0.8313\n",
      "Epoch 67/100\n",
      "166/166 [==============================] - 0s 120us/sample - loss: 0.4355 - accuracy: 0.8313\n",
      "Epoch 68/100\n",
      "166/166 [==============================] - 0s 73us/sample - loss: 0.4338 - accuracy: 0.8313\n",
      "Epoch 69/100\n",
      "166/166 [==============================] - 0s 98us/sample - loss: 0.4315 - accuracy: 0.8373\n",
      "Epoch 70/100\n",
      "166/166 [==============================] - 0s 103us/sample - loss: 0.4300 - accuracy: 0.8434\n",
      "Epoch 71/100\n",
      "166/166 [==============================] - 0s 116us/sample - loss: 0.4273 - accuracy: 0.8373\n",
      "Epoch 72/100\n",
      "166/166 [==============================] - 0s 141us/sample - loss: 0.4250 - accuracy: 0.8313\n",
      "Epoch 73/100\n",
      "166/166 [==============================] - 0s 105us/sample - loss: 0.4243 - accuracy: 0.8313\n",
      "Epoch 74/100\n",
      "166/166 [==============================] - 0s 122us/sample - loss: 0.4216 - accuracy: 0.8373\n",
      "Epoch 75/100\n",
      "166/166 [==============================] - 0s 101us/sample - loss: 0.4229 - accuracy: 0.8193\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 111us/sample - loss: 0.4201 - accuracy: 0.8253\n",
      "Epoch 77/100\n",
      "166/166 [==============================] - 0s 109us/sample - loss: 0.4170 - accuracy: 0.8434\n",
      "Epoch 78/100\n",
      "166/166 [==============================] - 0s 133us/sample - loss: 0.4141 - accuracy: 0.8373\n",
      "Epoch 79/100\n",
      "166/166 [==============================] - 0s 162us/sample - loss: 0.4119 - accuracy: 0.8434\n",
      "Epoch 80/100\n",
      "166/166 [==============================] - 0s 124us/sample - loss: 0.4140 - accuracy: 0.8253\n",
      "Epoch 81/100\n",
      "166/166 [==============================] - 0s 133us/sample - loss: 0.4107 - accuracy: 0.8313\n",
      "Epoch 82/100\n",
      "166/166 [==============================] - 0s 101us/sample - loss: 0.4067 - accuracy: 0.8434\n",
      "Epoch 83/100\n",
      "166/166 [==============================] - 0s 103us/sample - loss: 0.4095 - accuracy: 0.8373\n",
      "Epoch 84/100\n",
      "166/166 [==============================] - 0s 105us/sample - loss: 0.4020 - accuracy: 0.8494\n",
      "Epoch 85/100\n",
      "166/166 [==============================] - 0s 103us/sample - loss: 0.4058 - accuracy: 0.8253\n",
      "Epoch 86/100\n",
      "166/166 [==============================] - 0s 127us/sample - loss: 0.4033 - accuracy: 0.8434\n",
      "Epoch 87/100\n",
      "166/166 [==============================] - 0s 131us/sample - loss: 0.4007 - accuracy: 0.8434\n",
      "Epoch 88/100\n",
      "166/166 [==============================] - 0s 148us/sample - loss: 0.3980 - accuracy: 0.8494\n",
      "Epoch 89/100\n",
      "166/166 [==============================] - 0s 124us/sample - loss: 0.3969 - accuracy: 0.8494\n",
      "Epoch 90/100\n",
      "166/166 [==============================] - 0s 109us/sample - loss: 0.3955 - accuracy: 0.8313\n",
      "Epoch 91/100\n",
      "166/166 [==============================] - 0s 126us/sample - loss: 0.3918 - accuracy: 0.8434\n",
      "Epoch 92/100\n",
      "166/166 [==============================] - 0s 161us/sample - loss: 0.3910 - accuracy: 0.8373\n",
      "Epoch 93/100\n",
      "166/166 [==============================] - 0s 127us/sample - loss: 0.3893 - accuracy: 0.8373\n",
      "Epoch 94/100\n",
      "166/166 [==============================] - 0s 121us/sample - loss: 0.3896 - accuracy: 0.8373\n",
      "Epoch 95/100\n",
      "166/166 [==============================] - 0s 128us/sample - loss: 0.3885 - accuracy: 0.8554\n",
      "Epoch 96/100\n",
      "166/166 [==============================] - 0s 120us/sample - loss: 0.3853 - accuracy: 0.8434\n",
      "Epoch 97/100\n",
      "166/166 [==============================] - 0s 127us/sample - loss: 0.3834 - accuracy: 0.8434\n",
      "Epoch 98/100\n",
      "166/166 [==============================] - 0s 116us/sample - loss: 0.3812 - accuracy: 0.8494\n",
      "Epoch 99/100\n",
      "166/166 [==============================] - 0s 127us/sample - loss: 0.3818 - accuracy: 0.8494\n",
      "Epoch 100/100\n",
      "166/166 [==============================] - 0s 124us/sample - loss: 0.3819 - accuracy: 0.8614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19b054c1390>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs= 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d5e9059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "42/42 [==============================] - 0s 691us/sample - loss: 0.3488 - accuracy: 0.8810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3487624412491208, 0.88095236]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0afdfe",
   "metadata": {},
   "source": [
    "#### Predicting test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "678f45e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a9e79f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8182634 ],\n",
       "       [0.10038793],\n",
       "       [0.45297652],\n",
       "       [0.20819765],\n",
       "       [0.8373208 ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "555588f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161    1\n",
       "15     0\n",
       "73     0\n",
       "96     0\n",
       "166    1\n",
       "Name: 60, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14ecdfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## rounding y_pred to 0 and 1\n",
    "y_pred = np.round(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b2947bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22bbada",
   "metadata": {},
   "source": [
    "### Evaluating Model without dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a928fd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15,  1],\n",
       "       [ 4, 22]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1ab89190",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(33.0, 0.5, 'truth')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sn.heatmap(cm, annot= True, fmt= \"d\")\n",
    "plt.xlabel(\"prediction\")\n",
    "plt.ylabel(\"truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1b94c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.94      0.86        16\n",
      "           1       0.96      0.85      0.90        26\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.87      0.89      0.88        42\n",
      "weighted avg       0.89      0.88      0.88        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_model = classification_report(y_test, y_pred)\n",
    "print(classification_report_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0277742",
   "metadata": {},
   "source": [
    "---> detection of 1 is really important for militry purpose but there it is significantly high, but better then random gusess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19415af1",
   "metadata": {},
   "source": [
    "## Creating Model with Dropout regularization\n",
    "* for overcome from overfitting and improving performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "06d08b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_drop = keras.Sequential([\n",
    "        keras.layers.Dense(20 ,input_shape = (60, ), activation = \"relu\"),\n",
    "        keras.layers.Dropout(0.4),\n",
    "        keras.layers.Dense(1, activation= \"sigmoid\")\n",
    "    ])\n",
    "    \n",
    "model_drop.compile(\n",
    "    optimizer = \"adam\",\n",
    "    loss = \"binary_crossentropy\",\n",
    "    metrics = [\"accuracy\"]  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a42f2f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 166 samples\n",
      "Epoch 1/100\n",
      "166/166 [==============================] - 0s 678us/sample - loss: 0.7152 - accuracy: 0.5181\n",
      "Epoch 2/100\n",
      "166/166 [==============================] - 0s 149us/sample - loss: 0.6997 - accuracy: 0.5361\n",
      "Epoch 3/100\n",
      "166/166 [==============================] - 0s 133us/sample - loss: 0.7243 - accuracy: 0.4940\n",
      "Epoch 4/100\n",
      "166/166 [==============================] - 0s 204us/sample - loss: 0.6818 - accuracy: 0.5482\n",
      "Epoch 5/100\n",
      "166/166 [==============================] - 0s 152us/sample - loss: 0.6751 - accuracy: 0.5783\n",
      "Epoch 6/100\n",
      "166/166 [==============================] - 0s 143us/sample - loss: 0.7278 - accuracy: 0.4639\n",
      "Epoch 7/100\n",
      "166/166 [==============================] - 0s 144us/sample - loss: 0.6711 - accuracy: 0.5904\n",
      "Epoch 8/100\n",
      "166/166 [==============================] - 0s 151us/sample - loss: 0.6832 - accuracy: 0.5482\n",
      "Epoch 9/100\n",
      "166/166 [==============================] - 0s 149us/sample - loss: 0.6665 - accuracy: 0.5723\n",
      "Epoch 10/100\n",
      "166/166 [==============================] - 0s 131us/sample - loss: 0.6924 - accuracy: 0.5482\n",
      "Epoch 11/100\n",
      "166/166 [==============================] - 0s 133us/sample - loss: 0.6847 - accuracy: 0.5422\n",
      "Epoch 12/100\n",
      "166/166 [==============================] - 0s 145us/sample - loss: 0.6673 - accuracy: 0.5723\n",
      "Epoch 13/100\n",
      "166/166 [==============================] - 0s 145us/sample - loss: 0.6418 - accuracy: 0.6386\n",
      "Epoch 14/100\n",
      "166/166 [==============================] - 0s 124us/sample - loss: 0.6603 - accuracy: 0.6205\n",
      "Epoch 15/100\n",
      "166/166 [==============================] - 0s 131us/sample - loss: 0.6597 - accuracy: 0.6145\n",
      "Epoch 16/100\n",
      "166/166 [==============================] - 0s 138us/sample - loss: 0.6533 - accuracy: 0.6024\n",
      "Epoch 17/100\n",
      "166/166 [==============================] - 0s 125us/sample - loss: 0.6609 - accuracy: 0.5964\n",
      "Epoch 18/100\n",
      "166/166 [==============================] - 0s 138us/sample - loss: 0.6418 - accuracy: 0.6145\n",
      "Epoch 19/100\n",
      "166/166 [==============================] - 0s 133us/sample - loss: 0.6338 - accuracy: 0.6627\n",
      "Epoch 20/100\n",
      "166/166 [==============================] - 0s 134us/sample - loss: 0.6320 - accuracy: 0.6627\n",
      "Epoch 21/100\n",
      "166/166 [==============================] - 0s 128us/sample - loss: 0.6302 - accuracy: 0.6265\n",
      "Epoch 22/100\n",
      "166/166 [==============================] - 0s 153us/sample - loss: 0.6320 - accuracy: 0.7048\n",
      "Epoch 23/100\n",
      "166/166 [==============================] - 0s 130us/sample - loss: 0.6337 - accuracy: 0.6446\n",
      "Epoch 24/100\n",
      "166/166 [==============================] - 0s 139us/sample - loss: 0.6283 - accuracy: 0.6446\n",
      "Epoch 25/100\n",
      "166/166 [==============================] - 0s 125us/sample - loss: 0.6364 - accuracy: 0.6747\n",
      "Epoch 26/100\n",
      "166/166 [==============================] - 0s 140us/sample - loss: 0.6143 - accuracy: 0.7108\n",
      "Epoch 27/100\n",
      "166/166 [==============================] - 0s 135us/sample - loss: 0.6231 - accuracy: 0.6867\n",
      "Epoch 28/100\n",
      "166/166 [==============================] - 0s 130us/sample - loss: 0.6054 - accuracy: 0.7108\n",
      "Epoch 29/100\n",
      "166/166 [==============================] - 0s 120us/sample - loss: 0.6017 - accuracy: 0.7169\n",
      "Epoch 30/100\n",
      "166/166 [==============================] - 0s 130us/sample - loss: 0.6112 - accuracy: 0.7048\n",
      "Epoch 31/100\n",
      "166/166 [==============================] - 0s 138us/sample - loss: 0.6123 - accuracy: 0.6747\n",
      "Epoch 32/100\n",
      "166/166 [==============================] - 0s 142us/sample - loss: 0.6229 - accuracy: 0.6446\n",
      "Epoch 33/100\n",
      "166/166 [==============================] - 0s 134us/sample - loss: 0.6110 - accuracy: 0.6807\n",
      "Epoch 34/100\n",
      "166/166 [==============================] - 0s 136us/sample - loss: 0.5965 - accuracy: 0.7410\n",
      "Epoch 35/100\n",
      "166/166 [==============================] - 0s 143us/sample - loss: 0.5934 - accuracy: 0.7470\n",
      "Epoch 36/100\n",
      "166/166 [==============================] - 0s 113us/sample - loss: 0.6148 - accuracy: 0.6325\n",
      "Epoch 37/100\n",
      "166/166 [==============================] - 0s 138us/sample - loss: 0.6112 - accuracy: 0.6807\n",
      "Epoch 38/100\n",
      "166/166 [==============================] - 0s 130us/sample - loss: 0.5701 - accuracy: 0.7771\n",
      "Epoch 39/100\n",
      "166/166 [==============================] - 0s 105us/sample - loss: 0.5883 - accuracy: 0.6928\n",
      "Epoch 40/100\n",
      "166/166 [==============================] - 0s 111us/sample - loss: 0.5845 - accuracy: 0.7048\n",
      "Epoch 41/100\n",
      "166/166 [==============================] - 0s 122us/sample - loss: 0.5998 - accuracy: 0.6566\n",
      "Epoch 42/100\n",
      "166/166 [==============================] - 0s 103us/sample - loss: 0.5660 - accuracy: 0.7590\n",
      "Epoch 43/100\n",
      "166/166 [==============================] - 0s 120us/sample - loss: 0.5672 - accuracy: 0.7229\n",
      "Epoch 44/100\n",
      "166/166 [==============================] - 0s 115us/sample - loss: 0.5850 - accuracy: 0.6747\n",
      "Epoch 45/100\n",
      "166/166 [==============================] - 0s 108us/sample - loss: 0.5811 - accuracy: 0.7108\n",
      "Epoch 46/100\n",
      "166/166 [==============================] - 0s 117us/sample - loss: 0.5780 - accuracy: 0.6928\n",
      "Epoch 47/100\n",
      "166/166 [==============================] - 0s 108us/sample - loss: 0.5860 - accuracy: 0.6687\n",
      "Epoch 48/100\n",
      "166/166 [==============================] - 0s 117us/sample - loss: 0.5701 - accuracy: 0.6988\n",
      "Epoch 49/100\n",
      "166/166 [==============================] - 0s 143us/sample - loss: 0.5488 - accuracy: 0.7590\n",
      "Epoch 50/100\n",
      "166/166 [==============================] - 0s 146us/sample - loss: 0.5594 - accuracy: 0.7048\n",
      "Epoch 51/100\n",
      "166/166 [==============================] - 0s 143us/sample - loss: 0.5381 - accuracy: 0.7651\n",
      "Epoch 52/100\n",
      "166/166 [==============================] - 0s 136us/sample - loss: 0.5482 - accuracy: 0.7470\n",
      "Epoch 53/100\n",
      "166/166 [==============================] - 0s 172us/sample - loss: 0.5472 - accuracy: 0.7651\n",
      "Epoch 54/100\n",
      "166/166 [==============================] - 0s 141us/sample - loss: 0.5577 - accuracy: 0.7169\n",
      "Epoch 55/100\n",
      "166/166 [==============================] - 0s 176us/sample - loss: 0.5680 - accuracy: 0.7530\n",
      "Epoch 56/100\n",
      "166/166 [==============================] - 0s 141us/sample - loss: 0.5497 - accuracy: 0.7169\n",
      "Epoch 57/100\n",
      "166/166 [==============================] - 0s 145us/sample - loss: 0.5497 - accuracy: 0.7289\n",
      "Epoch 58/100\n",
      "166/166 [==============================] - 0s 150us/sample - loss: 0.5351 - accuracy: 0.7771\n",
      "Epoch 59/100\n",
      "166/166 [==============================] - 0s 125us/sample - loss: 0.5558 - accuracy: 0.7349\n",
      "Epoch 60/100\n",
      "166/166 [==============================] - 0s 114us/sample - loss: 0.5247 - accuracy: 0.7952\n",
      "Epoch 61/100\n",
      "166/166 [==============================] - 0s 103us/sample - loss: 0.5531 - accuracy: 0.7590\n",
      "Epoch 62/100\n",
      "166/166 [==============================] - 0s 166us/sample - loss: 0.5300 - accuracy: 0.7771\n",
      "Epoch 63/100\n",
      "166/166 [==============================] - 0s 171us/sample - loss: 0.5525 - accuracy: 0.7229\n",
      "Epoch 64/100\n",
      "166/166 [==============================] - 0s 179us/sample - loss: 0.5259 - accuracy: 0.7590\n",
      "Epoch 65/100\n",
      "166/166 [==============================] - 0s 174us/sample - loss: 0.5242 - accuracy: 0.7892\n",
      "Epoch 66/100\n",
      "166/166 [==============================] - 0s 172us/sample - loss: 0.5329 - accuracy: 0.7590\n",
      "Epoch 67/100\n",
      "166/166 [==============================] - 0s 185us/sample - loss: 0.5211 - accuracy: 0.7952\n",
      "Epoch 68/100\n",
      "166/166 [==============================] - 0s 167us/sample - loss: 0.5259 - accuracy: 0.7470\n",
      "Epoch 69/100\n",
      "166/166 [==============================] - 0s 204us/sample - loss: 0.5146 - accuracy: 0.7831\n",
      "Epoch 70/100\n",
      "166/166 [==============================] - 0s 201us/sample - loss: 0.5402 - accuracy: 0.7590\n",
      "Epoch 71/100\n",
      "166/166 [==============================] - 0s 137us/sample - loss: 0.5375 - accuracy: 0.7771\n",
      "Epoch 72/100\n",
      "166/166 [==============================] - 0s 154us/sample - loss: 0.5260 - accuracy: 0.7831\n",
      "Epoch 73/100\n",
      "166/166 [==============================] - 0s 158us/sample - loss: 0.5176 - accuracy: 0.7410\n",
      "Epoch 74/100\n",
      "166/166 [==============================] - 0s 158us/sample - loss: 0.5047 - accuracy: 0.7771\n",
      "Epoch 75/100\n",
      "166/166 [==============================] - 0s 132us/sample - loss: 0.5159 - accuracy: 0.7470\n",
      "Epoch 76/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166/166 [==============================] - 0s 129us/sample - loss: 0.5285 - accuracy: 0.7349\n",
      "Epoch 77/100\n",
      "166/166 [==============================] - 0s 124us/sample - loss: 0.5129 - accuracy: 0.7410\n",
      "Epoch 78/100\n",
      "166/166 [==============================] - 0s 104us/sample - loss: 0.5048 - accuracy: 0.8253\n",
      "Epoch 79/100\n",
      "166/166 [==============================] - 0s 121us/sample - loss: 0.5210 - accuracy: 0.8012\n",
      "Epoch 80/100\n",
      "166/166 [==============================] - 0s 121us/sample - loss: 0.5118 - accuracy: 0.7711\n",
      "Epoch 81/100\n",
      "166/166 [==============================] - 0s 118us/sample - loss: 0.4953 - accuracy: 0.8434\n",
      "Epoch 82/100\n",
      "166/166 [==============================] - 0s 134us/sample - loss: 0.4772 - accuracy: 0.7771\n",
      "Epoch 83/100\n",
      "166/166 [==============================] - 0s 110us/sample - loss: 0.5032 - accuracy: 0.7892\n",
      "Epoch 84/100\n",
      "166/166 [==============================] - 0s 119us/sample - loss: 0.4756 - accuracy: 0.8193\n",
      "Epoch 85/100\n",
      "166/166 [==============================] - 0s 125us/sample - loss: 0.4957 - accuracy: 0.7892\n",
      "Epoch 86/100\n",
      "166/166 [==============================] - 0s 114us/sample - loss: 0.5091 - accuracy: 0.7771\n",
      "Epoch 87/100\n",
      "166/166 [==============================] - 0s 113us/sample - loss: 0.4931 - accuracy: 0.8012\n",
      "Epoch 88/100\n",
      "166/166 [==============================] - 0s 121us/sample - loss: 0.5080 - accuracy: 0.7410\n",
      "Epoch 89/100\n",
      "166/166 [==============================] - 0s 115us/sample - loss: 0.5023 - accuracy: 0.7952\n",
      "Epoch 90/100\n",
      "166/166 [==============================] - 0s 129us/sample - loss: 0.5112 - accuracy: 0.7590\n",
      "Epoch 91/100\n",
      "166/166 [==============================] - 0s 117us/sample - loss: 0.5033 - accuracy: 0.7229\n",
      "Epoch 92/100\n",
      "166/166 [==============================] - 0s 120us/sample - loss: 0.4853 - accuracy: 0.7892\n",
      "Epoch 93/100\n",
      "166/166 [==============================] - 0s 107us/sample - loss: 0.5072 - accuracy: 0.7831\n",
      "Epoch 94/100\n",
      "166/166 [==============================] - 0s 118us/sample - loss: 0.4659 - accuracy: 0.8012\n",
      "Epoch 95/100\n",
      "166/166 [==============================] - 0s 125us/sample - loss: 0.5012 - accuracy: 0.7771\n",
      "Epoch 96/100\n",
      "166/166 [==============================] - 0s 126us/sample - loss: 0.4792 - accuracy: 0.7711\n",
      "Epoch 97/100\n",
      "166/166 [==============================] - 0s 108us/sample - loss: 0.4777 - accuracy: 0.8072\n",
      "Epoch 98/100\n",
      "166/166 [==============================] - 0s 122us/sample - loss: 0.4799 - accuracy: 0.8072\n",
      "Epoch 99/100\n",
      "166/166 [==============================] - 0s 120us/sample - loss: 0.4788 - accuracy: 0.7651\n",
      "Epoch 100/100\n",
      "166/166 [==============================] - 0s 132us/sample - loss: 0.4518 - accuracy: 0.8193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19b1d1d05c0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_drop.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26452fba",
   "metadata": {},
   "source": [
    "--> so it this case dropout is not prforming enough well\n",
    "* for epochs 1000\n",
    "1. 166/166 [==============================] - 0s 119us/sample - loss: 0.1671 - accuracy: 0.9398\n",
    "\n",
    "\n",
    "* epochs = 100\n",
    "* 166/166 [==============================] - 0s 132us/sample - loss: 0.4518 - accuracy: 0.8193\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d3f54",
   "metadata": {},
   "source": [
    "#### evaluate model_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c9d5fa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "42/42 [==============================] - 0s 747us/sample - loss: 0.4235 - accuracy: 0.9048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4234867110138848, 0.9047619]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_drop.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3e2388",
   "metadata": {},
   "source": [
    "--> so here dropout is making the model wrost\n",
    "* for epochs = 1000\n",
    "1. [0.7983650508381072, 0.33333334]\n",
    "\n",
    "* for epochs = 100\n",
    "1. [0.4234867110138848, 0.9047619]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "482570af",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### predictions on test set using dropout model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e5f66a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.44451797],\n",
       "       [0.45284328],\n",
       "       [0.36624008],\n",
       "       [0.4814228 ],\n",
       "       [0.4494161 ]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_d = model.predict(X_test)\n",
    "y_pred_d[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9a3b4312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_d = np.round(y_pred_d)\n",
    "y_pred_d[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4091800e",
   "metadata": {},
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7d9ccc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  3],\n",
       "       [25,  1]], dtype=int64)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_d = confusion_matrix(y_test, y_pred_d)\n",
    "cm_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cbaec05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.81      0.48        16\n",
      "           1       0.25      0.04      0.07        26\n",
      "\n",
      "    accuracy                           0.33        42\n",
      "   macro avg       0.30      0.43      0.27        42\n",
      "weighted avg       0.29      0.33      0.22        42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classification_report_drop = classification_report(y_test, y_pred_d)\n",
    "print(classification_report_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e19647",
   "metadata": {},
   "source": [
    "---> Very poor performance with dropout model, can not approve for launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29286bbb",
   "metadata": {},
   "source": [
    "###### Diggig the prediction by dropout model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "39a26431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]], dtype=float32),\n",
       " 161    1\n",
       " 15     0\n",
       " 73     0\n",
       " 96     0\n",
       " 166    1\n",
       " 9      0\n",
       " 100    1\n",
       " 135    1\n",
       " 18     0\n",
       " 148    1\n",
       " 171    1\n",
       " 30     0\n",
       " 155    1\n",
       " 180    1\n",
       " 125    1\n",
       " 197    1\n",
       " 164    1\n",
       " 190    1\n",
       " 84     0\n",
       " 75     0\n",
       " 124    1\n",
       " 170    1\n",
       " 104    1\n",
       " 101    1\n",
       " 69     0\n",
       " 25     0\n",
       " 95     0\n",
       " 16     0\n",
       " 141    1\n",
       " 185    1\n",
       " 154    1\n",
       " 68     0\n",
       " 66     0\n",
       " 120    1\n",
       " 147    1\n",
       " 98     1\n",
       " 138    1\n",
       " 167    1\n",
       " 45     0\n",
       " 113    1\n",
       " 65     0\n",
       " 178    1\n",
       " Name: 60, dtype: int64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_d, y_test   # very bed prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9e24be12",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-101-909074b3e048>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-101-909074b3e048>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    <h1 align = \"center\"> Model For Launch </h1>\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<h1 align = \"center\"> Model For Launch </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff7ee03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
